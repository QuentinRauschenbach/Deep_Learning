{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "150fb896",
   "metadata": {
    "id": "150fb896"
   },
   "source": [
    "Let's start off by importing some packages we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b772eb",
   "metadata": {
    "id": "82b772eb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bacee54",
   "metadata": {
    "id": "4bacee54"
   },
   "source": [
    "* Create some random data of size (B, C, H) where B is a batch dimension, C is number of channels (start with 1) and H is height.\n",
    "* Create a 1D convolution layer `L` using `torch.nn.Conv1d`, with a 3-element filter. Start off with 1 input and output.\n",
    "* Inspect the free parameters of your layer using its `.weight` and `.bias` fields.\n",
    "* How many dimensions does `L.weight` have, and what do they all represent?\n",
    "* Set the bias and weights to all be zero. Set one weight to be 2. Apply your convolution layer to an input tensor and describe what happens.\n",
    "* How many dimensions did your input tensor need to have, and what does each represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb59a6e8",
   "metadata": {
    "id": "eb59a6e8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "609abc03",
   "metadata": {
    "id": "609abc03"
   },
   "source": [
    "* Create and test a convolution with multiple input and output channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a18a842",
   "metadata": {
    "id": "3a18a842"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d36b067",
   "metadata": {
    "id": "1d36b067"
   },
   "source": [
    "* Repeat the previous problem, but now for random input data of size (B, C, H, W) and a `torch.nn.Conv2d` layer with a 3x3 filter.\n",
    "* Go up to at least 2 inputs and outputs. What does each dimension of `L.weight` and `L.bias` mean now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1870c19",
   "metadata": {
    "id": "a1870c19"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d05a3d8",
   "metadata": {
    "id": "9d05a3d8"
   },
   "source": [
    "Experiment with different modes for the `padding` argument to the Conv2d class. See the documentation [here](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7473f9f3",
   "metadata": {
    "id": "7473f9f3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cb687d5",
   "metadata": {
    "id": "5cb687d5"
   },
   "source": [
    "* Create and test a `torch.nn.MaxPool2d` layer. Try several window sizes.\n",
    "* Do the same for a `torch.nn.AvgPool2d` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f513060",
   "metadata": {
    "id": "9f513060"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8aa9e6f",
   "metadata": {
    "id": "d8aa9e6f"
   },
   "source": [
    "Try out the hyperbolic tangent function `torch.tanh`:\n",
    "* Plot its output as a function of its input in increments of 0.01 from -10 to 10\n",
    "* Plot the ReLU function on the same graph for comparison.\n",
    "* Can you speculate as to when one or the other activation function might be preferable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c40374",
   "metadata": {
    "id": "92c40374"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5afff069",
   "metadata": {
    "id": "5afff069"
   },
   "source": [
    "Download the sea surface temperature and upper ocean heat content fields for ENSO prediction. If you like, you can add these files to your google drive or local filesystem in the same place as your copy of this notebook, so that you won't have to install gdown and download them in the future whenever you restart this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff09801",
   "metadata": {
    "id": "5ff09801"
   },
   "outputs": [],
   "source": [
    "# ! executes shell commands instead of python commands\n",
    "\n",
    "# library for downloading files from google drive.\n",
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf5e54",
   "metadata": {
    "id": "d5cf5e54"
   },
   "outputs": [],
   "source": [
    "# enso index data from previous exercises\n",
    "!gdown https://drive.google.com/uc?id=1FUb-2lcAd0Y1ULjx5jB6UTMNDmGy3vZA \n",
    "    \n",
    "# sea surface temperature and upper ocean heat content\n",
    "!gdown https://drive.google.com/uc?id=1TUFl4l4iEyIKTY1UnRD3G7fKElNMXjwo\n",
    "!gdown https://drive.google.com/uc?id=1PkDoopBJdYdiVt_clxnbDfuDGJGM0QSY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441fc863",
   "metadata": {
    "id": "441fc863"
   },
   "source": [
    "Download the non-PCA data for ENSO prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ba944b",
   "metadata": {
    "id": "46ba944b"
   },
   "source": [
    "Now load the sea surface temperature data (`sst.nc`) using xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859846e8",
   "metadata": {
    "id": "859846e8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b0bebb8",
   "metadata": {
    "id": "4b0bebb8"
   },
   "source": [
    "Convert the sea surface temperatures to a tensor with 32 bit floating point data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340a9689",
   "metadata": {
    "id": "340a9689"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa055182",
   "metadata": {
    "id": "fa055182"
   },
   "source": [
    "Load the target ENSO index data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38054e97",
   "metadata": {
    "id": "38054e97"
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "with np.load('enso_and_pca.npz') as data:    \n",
    "    enso_index = data['y']  # 3-month-moving-averaged Nino3.4 index\n",
    "    t_enso = data['t']  # months since jan. 1 1960 for the center of each 3-month window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb1ab14",
   "metadata": {
    "id": "fbb1ab14"
   },
   "source": [
    "The ENSO index is a 3-month moving average, and we'll try to predict it from temperature fields of of the ocean 3 months before the center of the window. That means we have to throw out\n",
    "* The first few enso_index values, since we don't have ocean temperature fields 3 months earlier\n",
    "* The last few temperature fields, since we don't have enso_index values 3 months later\n",
    "\n",
    "Construct PyTorch tensors x and y for the temperature field in `sst` and the enso_index, so that `x[i]` was measured 3 months earlier than `y[i]`. Use as much of the data as you can, while dealing with the above-mentioned edge cases. To figure out the timing, compare `t_enso` and `sst.time`. Make sure `x` and `y` have a dtype of 32 bit floating point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed6757",
   "metadata": {
    "id": "23ed6757"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "835f5cef",
   "metadata": {
    "id": "835f5cef"
   },
   "source": [
    "Normalize `x` and `y` to have a mean of zero and a standard deviation of 1. Don't normalize `x` separately for each location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9751083e",
   "metadata": {
    "id": "9751083e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c442f8fc",
   "metadata": {
    "id": "c442f8fc"
   },
   "source": [
    "To use 2d convolutions, we need to have a channel dimension for our input tensor, which should be of size `(n_batch, n_channels, n_rows, n_columns)`. Use `reshape` or `unsqueeze` to add a channel axis of size 1 to `x`. It should be in position one, after the batch axis (position 0) but before the spatial dimensions.\n",
    "\n",
    "Similarly, change `y` to have a second dimension of size 1, so that it has two dimensions total. This way it will match the output coming from out network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6528f592",
   "metadata": {
    "id": "6528f592"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c1dfc37",
   "metadata": {
    "id": "6c1dfc37"
   },
   "source": [
    "Now create a TensorDataset and a dataloader from these tensors, just as we did in last week's exercises. Following Ham et al., we'll use a batch size of 400.\n",
    "\n",
    "We won't split training and testing data this time so we can focus on what's new, but keep in mind this is normally the right practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49486e0",
   "metadata": {
    "id": "b49486e0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14dcbb77",
   "metadata": {
    "id": "14dcbb77"
   },
   "source": [
    "Create a class defining your convolutional network.\n",
    "\n",
    "Add code to the `__init__` method to define the layers, and add code to the `forward` method to use them.\n",
    "* Start with a convolution with 30 output channels, and 'same' padding so that the image width and height are the same in the input and output. Use filters with a height of 4 and width of 8.\n",
    "* Then use a `tanh` nonlinearity\n",
    "* Next use a 2x2 maxpooling layer\n",
    "* Then another convolution with 30 outputs, filter height 2 and filter width 4.\n",
    "* Next another `tanh`\n",
    "* Now another 2x2 maxpooling layer\n",
    "* A third convolution with 30 outputs, filter height 2 and filter width 4\n",
    "* Now reshape the output so the batch dimension is unchanged, but all other dimensions are combined into one.\n",
    "* Now use a fully connected layer with 30 outputs\n",
    "* Now a `tanh`\n",
    "* Now a second fully connected layer with 1 output: that's $\\hat our y$, which our forward method should return!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9223ccd7",
   "metadata": {
    "id": "9223ccd7"
   },
   "outputs": [],
   "source": [
    "# build the convnet from the paper\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self, input_channels=1, hidden_channels=30, hidden_units=30, n_outputs=1):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        #self.first_conv = \n",
    "        # ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.first_conv(x)\n",
    "        #x = ...\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa10a998",
   "metadata": {
    "id": "fa10a998"
   },
   "source": [
    "Create an instance of your network class.\n",
    "\n",
    "Pass a single batch of data through the network, and check that everything makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f8639d",
   "metadata": {
    "id": "65f8639d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0df0480",
   "metadata": {
    "id": "c0df0480"
   },
   "source": [
    "How many free parameters does your network have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474dc776",
   "metadata": {
    "id": "474dc776"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3118951",
   "metadata": {
    "id": "d3118951"
   },
   "source": [
    "Write a training loop.\n",
    "\n",
    "To avoid having to loop over trainable parameters, we'll use the optimizer class. We tell it once which parameters to update and what the learning rate is, then we can simply call `optimizer.step()` after calling `loss.backward()`, and all the parameters will be updated appropriately.\n",
    "\n",
    "**Important**: For the optimizer to work properly each convolutional or fully connected layer must be an attribute of your `net.object`, for example by calling `self.layer = torch.nn.Conv2d(...)` in your network's `__init__` method. If you want to use a list instead that's ok, but in that case include `self.layers_list = torch.nn.ModuleList(layers_list)` to make sure that the optimizer finds the parameters of these layers.\n",
    "\n",
    "Train your network over 100 epochs with a learning rate of 0.001. Keep track of the loss values and plot them at the end\n",
    "\n",
    "**Optional**: if you want training to go more quickly, use a GPU instance. Generally you'll get it for 12 hours, and it will disconnect if it goes idle for an hour.\n",
    "* In the colab menu, go to Runtime -> Change runtime type -> Hardware accelerator -> GPU and restart the colab notebook.\n",
    "* Use `print(torch.cuda.get_device_name(0))` to figure out what kind of GPU you have\n",
    "* Define `device = torch.device('cuda:0')`\n",
    "* Call `net = net.to(device)` and `x = x.to(device)` and `y = y.to(device)`. Create a new Dataset and Dataloader, and start training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee0b413",
   "metadata": {
    "id": "8ee0b413"
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "learning_rate = 0.001  # adjust this if necessary\n",
    "\n",
    "lossfunc = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    for batch_index, batch in enumerate(data_loader):\n",
    "        # note that each time this loop is run through, the order of the data is randomly permuted!\n",
    "        x_batch, y_batch = batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #yhat_batch = ...\n",
    "        #loss = lossfunc( ... )\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()  # this will update parameters using torch.no_grad()\n",
    "        optimzer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151db14c",
   "metadata": {
    "id": "151db14c"
   },
   "source": [
    "Plot the predictions $\\hat y$ against the targets $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ce28fe",
   "metadata": {
    "id": "c8ce28fe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2dbdfb34",
   "metadata": {
    "id": "2dbdfb34"
   },
   "source": [
    "Calculate the Pearson's correlation between your prediction and the target over the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a74c37d",
   "metadata": {
    "id": "7a74c37d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6aa122ea",
   "metadata": {
    "id": "6aa122ea"
   },
   "source": [
    "Load `hc.nc`, containing heat content of the upper ocean, and use it as a second input channel to the network. Does this improve performance on the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1dc648",
   "metadata": {
    "id": "ff1dc648"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8845110",
   "metadata": {
    "id": "e8845110"
   },
   "source": [
    "**Optional** Include two previous time steps as input for a total of 6 input channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05812c87",
   "metadata": {
    "id": "05812c87"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fba04d5d",
   "metadata": {
    "id": "fba04d5d"
   },
   "source": [
    "In this case, we were able to load all of the training data into memory and store it in pytorch tensors. What if the training data were too big to fit into memory all at once? What could we do then? Propose a potential solution to this problem, and describe in words (or if you like, code) roughly how it would work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f470f19",
   "metadata": {
    "id": "3f470f19"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
