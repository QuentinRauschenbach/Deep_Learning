{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f6bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some libraries we'll need\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec31981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf865ddf",
   "metadata": {},
   "source": [
    "Suppose we have a neural network with two inputs, two hidden layers of 8 units each, ReLU activations (except for $\\hat y = f_\\text{out}(h_L)$) and one output.\n",
    "\n",
    "* Write an algorithm using python to compute the NN output $\\hat y$ given the input and the free parameters (weights and biases)\n",
    "* Write an algorithm to compute derivatives of the least squares loss $(y - \\hat y)^2$ with respect to all free parameters of the NN. This should be a python function that takes the free parameters, NN inputs $x$ and NN training targets $y$ as its own inputs. **Don't use automatic differntiation, finite differences or symbolic math tools: do the derivatives by hand!**\n",
    "* **Optional challenge problem (+25% of assignment max. points), skip for now if you get stuck**: Extend your derivative computation algorithm to work with any number of hidden layers, all of which can have different numbers of hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b5ac1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ffd3fa8",
   "metadata": {},
   "source": [
    "We'll now download a data file and extract 3 variables. Each is a NumPy array:\n",
    "* 'y' contains monthly values of the [Nino 3.4 ENSO index](https://climatedataguide.ucar.edu/climate-data/nino-sst-indices-nino-12-3-34-4-oni-and-tni), describing the [El Nino Southern Oscillation](https://en.wikipedia.org/wiki/El_Ni%C3%B1o%E2%80%93Southern_Oscillation) over time.\n",
    "* `x` contains empirical orthogonal function coefficients that describe the ocean temperature at the sea surface and the depth average over the top 300 meters, in the Indo-Pacific, North-Pacific and Atlantic regions. Essentially, the numbers in each row of $x$ summarize temperatures in the upper ocean across the globe. For details of how these are calculated, you can consult [this paper](https://www.nature.com/articles/s41586-019-1559-7) by Ham et al. Each row of `x` describes the ocean state 2 months before the corresponding element of `y`.\n",
    "* `t` contains the time in months since Jan. 1 1970 for each value of `y`. We can use this for plotting results but it won't appear in our calculations otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a64f4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library for downloading files from google drive.\n",
    "!pip install gdown  \n",
    "\n",
    "# download the data\n",
    "!gdown https://drive.google.com/uc?id=1FUb-2lcAd0Y1ULjx5jB6UTMNDmGy3vZA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969ace86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "with np.load('enso_and_pca.npz') as data:\n",
    "    t, x, y = data['t'], data['x'], data['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836d1aac",
   "metadata": {},
   "source": [
    "Normalize `y` and each column of `x` so they all have a mean of zero and a standard deviation of one. Overwrite the original variables `x,y` with the normalized ones, but keep track of what you've done, so you can convert your predictions back into original data units later on if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90926ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0abec2f5",
   "metadata": {},
   "source": [
    "Last time we wrote a loop by hand to cycle over the data. However, pytorch provides great utilities to do this for us, which will let us focus on what's new in each lesson instead. In particular, we're going to use the PyTorch Dataset and DataLoader classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba056261",
   "metadata": {
    "id": "ba056261"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create PyTorch tensors for our inputs and target outputs\n",
    "xt = torch.tensor(x)\n",
    "yt = torch.tensor(y)\n",
    "\n",
    "# use the first 1100 time points as training data\n",
    "xt_train, yt_train = xt[:1100], yt[:1100]\n",
    "\n",
    "# save the remaining data for testing\n",
    "xt_test, yt_test = xt[1100:], yt[1100:]\n",
    "\n",
    "dataset = TensorDataset(xt_train, yt_train)  # combine the inputs and outputs into a PyTorch Dataset object\n",
    "# create a dataloader to serve up batches of 8 data point for training\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60572884",
   "metadata": {
    "id": "60572884"
   },
   "source": [
    "Now let's get the first 'batch' of data from the dataloader, and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b27757",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96b27757",
    "outputId": "d7ae234d-d634-4e99-e22e-8d9502956a3d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x_batch, y_batch in data_loader:\n",
    "    print('shape of x_batch: {0}'.format(x_batch.shape) )\n",
    "    print('shape of y_batch: {0}'.format(y_batch.shape) )  \n",
    "    break  # if we didn't have this line, the for loop would cycle through all the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505a8900",
   "metadata": {},
   "source": [
    "Now use a for loop, iterating over batches from the data loader, to fit a linear regression $\\hat y = x\\cdot \\beta$ with least squares loss, by stochastic gradient descent. Refer back to the code you wrote from the last set of exercises for guidance (and feel free to copy-paste from your own previous homework).\n",
    "* Add an extra column of ones to `x` to incorporate a constant term\n",
    "* Don't forget to set `requires_grad = True` when initializing `beta`\n",
    "* Choose how to initialize $\\beta$. All zeros? Random? Does it make much of a difference here?\n",
    "* As before, don't forget to use `with torch.no_grad():` when updating parameters\n",
    "* Store the loss at each iteration of the loop.\n",
    "* Does the loss decrease on the training data? Do you get a positive correlation between $y$ and $\\hat y$ at the end on the training data?\n",
    "* See if you can get a better result by adjusting the initialization of free parameters or the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c812da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ea3cda8",
   "metadata": {},
   "source": [
    "Now repeat the process, but instead of a linear model we'll use a neural network with two hidden layers of 32 units each, and the ReLU activation function $\\phi(z) = \\max(0, z)$. Again we'll use least squares loss and a batch size of 8.\n",
    "\n",
    "**Don't** use PyTorch's built-in classes for this just yet. Instead:\n",
    "* Define tensors of the correct size and data type (check `x.dtype`) for each variable containing free parameters (weights and biases) of your neural network. Remember to set `requires_grad = True` where needed.\n",
    "* For each batch of data, compute the hidden state activations $h_1$ as a function of $W_1, b_1, x$\n",
    "* Then compute $h_2$, $\\hat y$ and $e=\\ell(y, \\hat y)$.\n",
    "* Call `e.backward()` to compute derivatives of $e$ with respect to the free variables of your neural network, using backward-mode differentiation.\n",
    "* Now update all free parameters based on the computed derivatives and the learning rate, just as you did for the linear regression example. Remember to use `with torch.no_grad():`\n",
    "* Plot the convergence of the loss function over iterations. Are you getting a better fit than with the linear regression?\n",
    "* How did you initialize? Does it make a difference now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35233fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb8893b4",
   "metadata": {},
   "source": [
    "Now plot the target ENSO 3.4 index values $y$ against the predictions from the linear regression and the neural network. Clearly mark the boundary between training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c66c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d06f5199",
   "metadata": {},
   "source": [
    "What are mean square errors for the NN and linear regression, in original data units, on the testing and training data? How about Pearson's correlation coefficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bc2080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1415976",
   "metadata": {},
   "source": [
    "Now we'll do the same thing, but defining a python object class for our neural network, and some built-in PyTorch classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e55053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_inputs=46, n_outputs=1, n_hidden=32):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(n_inputs, n_hidden)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_outputs)\n",
    "     \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f2b21e",
   "metadata": {},
   "source": [
    "As described in the first two lectures, we are now building our neural net out of composable functions that we can stack together like legos.\n",
    "\n",
    "PyTorch also lets us iterate in loops over the trainable parameters of the model. Note how `requires_grad` was correctly set by default for the learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3909bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "for name, param in net.named_parameters():\n",
    "    print(f'{name}: requires_grad={param.requires_grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e3ba79",
   "metadata": {},
   "source": [
    "When we use our network object like a function, the input gets passed to the `forward()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfa6215",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(net(torch.rand(46)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae38f0f",
   "metadata": {},
   "source": [
    "We can also provide it with multiple inputs to process independently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930f5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net(torch.rand(4,46)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ffafd4",
   "metadata": {},
   "source": [
    "Now redo the training loop from before, but now update the parameters using an inner loop over `net.named_parameters` (remember to use `torch.no_grad()` when updating parameters, and to zero out the derivatives after you do so).\n",
    "\n",
    "Instead of calling `var.grad.zero()` on each trainable variable, we can now also simply call `net.zero_grad()` once.\n",
    "\n",
    "Look up the default initialization of weights and biases in the `nn.Linear` objects you used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d300ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8698bfb0",
   "metadata": {},
   "source": [
    "Change the `Net()` class above to allow an adjustable number of hidden layers to be specified when initializing the object, and to specify a different number of hidden units for each hidden layer. Can you get better results on the testing data by adjusting this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f796dcc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
